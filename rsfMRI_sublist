import pandas as pd # to read/manipulate/write data from files
import numpy as np # to manipulate data/generate random numbers
import plotly.express as px # interactive visualizations
import seaborn as sns # static visualizations
import matplotlib.pyplot as plt # fine tune control over visualizations
import json

from pathlib import Path # represent and interact with directories/folders in the operating system
from collections import namedtuple # structure data in an easy to consume way

import requests # retrieve data from an online source

# save directory we downloaded the ABCD data to `data_path`
data_path = Path("/home/jovyan/ABCD3")
# glob (match) all text files in the `data_path` directory
files = sorted(data_path.glob("*.txt"))

# We store the info in 4 different Python datatypes
data_elements = []
data_structures = {}
event_names = set()
StructureInfo = namedtuple("StructureInfo", field_names=["description", "eventnames"])
for text_file in files:
    # Extract data structure from filename
    data_structure = Path(text_file).name.split('.txt')[0]
    
    # Read the data structure and capture all the elements from the file
    # Note this could have been done using the data returned from the NDA API
    # We are using pandas to read both the first and second rows of the file as the header
    # Note: by convention dataframe variables contain `df` in the name.
    data_structure_df = pd.read_table(text_file, header=[0, 1], nrows=0)
    for data_element, metadata in data_structure_df.columns.values.tolist():
        data_elements.append([data_element, metadata, data_structure])

    
    # (Optional) Retrieve the eventnames in each structure. Some structures were only collected
    # at baseline while others were collected at specific or multiple timepoints
    events_in_structure = None
    if any(['eventname' == data_element for data_element in data_structure_df.columns.levels[0]]):
        # Here we are skipping the 2nd row of the file containing description using skiprows
        possible_event_names_df = pd.read_table(text_file, skiprows=[1], usecols=['eventname'])
        events_in_structure = possible_event_names_df.eventname.unique().tolist()
        event_names.update(events_in_structure)
        
    # (Optional) Retrieve the title for the structure using the NDA API
    rinfo = requests.get(f"https://nda.nih.gov/api/datadictionary/datastructure/{data_structure}").json()
    data_structures[data_structure] = StructureInfo(description=rinfo["title"] if "title" in rinfo else None,
                                                    eventnames=events_in_structure)

# Convert to a Pandas dataframe
data_elements_df = pd.DataFrame(data_elements, columns=["element", "description", "structure"])
      
data_elements_df.to_csv("data_elements.tsv", sep="\t", index=None)


common = ["subjectkey", "interview_date", "interview_age", "eventname", "sex"]
demographic = ['rel_family_id']
clinical = ['medhx_6i', 'medhx_6p', 'medhx_6j', 'medhx_2m', 'medhx_2h','medhx_2f','medhx_2c','ksads_4_826_p', 
            'ksads_4_826_t','ksads_4_827_p','ksads_4_827_t', 'ksads_4_828_p', 'ksads_4_828_t','ksads_4_829_p',
            'ksads_4_829_t','ksads_4_849_p','ksads_4_849_t','ksads_4_850_p','ksads_4_850_t','ksads_4_851_p',
            'ksads_4_851_t','ksads_4_852_p','ksads_4_852_t']
exclusions = ["imgincl_rsfmri_include",]
imaging = ["rsfmri_cor_ngd_fopa_scs_aglh", "rsfmri_cor_ngd_fopa_scs_agrh"]
data_elements_of_interest = clinical + demographic + exclusions + imaging


structures2read = {}
for element in data_elements_of_interest:
    item = data_elements_df.query(f"element == '{element}'").structure.values[0]
    if item not in structures2read:
        structures2read[item] = []
    structures2read[item].append(element)


all_df = None
for structure, elements in structures2read.items():
    data_structure_filtered_df = pd.read_table(data_path / f"{structure}.txt", skiprows=[1], low_memory=False, usecols=common + elements)
    data_structure_filtered_df = data_structure_filtered_df.query("eventname == 'baseline_year_1_arm_1'")
    if all_df is None:
        all_df =  data_structure_filtered_df[["subjectkey", "interview_date", "interview_age", "sex"] + elements]
    else:
        all_df = all_df.merge( data_structure_filtered_df[['subjectkey'] + elements], how='outer')
      
all_df[all_df.duplicated('subjectkey', keep=False)]
all_df = all_df.dropna()
all_df.shape, all_df.subjectkey.unique().shape

#exclude if any ksads=1
ksads_df = all_df[~all_df["ksads_4_826_p"].isin([1])] #hallucinations present
ksads_df= ksads_df[~ksads_df["ksads_4_826_t"].isin([1])] #hallucinations present
ksads_df= ksads_df[~ksads_df["ksads_4_827_p"].isin([1])] #hallucinations past
ksads_df= ksads_df[~ksads_df["ksads_4_827_t"].isin([1])] #hallucinations past
ksads_df= ksads_df[~ksads_df["ksads_4_828_p"].isin([1])] #delusions present
ksads_df= ksads_df[~ksads_df["ksads_4_828_t"].isin([1])] #delusions present
ksads_df= ksads_df[~ksads_df["ksads_4_829_p"].isin([1])] #delusions past
ksads_df= ksads_df[~ksads_df["ksads_4_829_t"].isin([1])] #delusions past
ksads_df= ksads_df[~ksads_df["ksads_4_849_p"].isin([1])] #assoc. psychotic symptoms present
ksads_df= ksads_df[~ksads_df["ksads_4_849_t"].isin([1])] #assoc. psychotic symptoms present
ksads_df= ksads_df[~ksads_df["ksads_4_850_p"].isin([1])] #assoc. psychotic symptoms past
ksads_df= ksads_df[~ksads_df["ksads_4_850_t"].isin([1])] #assoc. psychotic symptoms past
ksads_df= ksads_df[~ksads_df["ksads_4_851_p"].isin([1])] #diagnosis scizophrenia spectrum present
ksads_df= ksads_df[~ksads_df["ksads_4_851_t"].isin([1])] #diagnosis scizophrenia spectrum present
ksads_df= ksads_df[~ksads_df["ksads_4_852_p"].isin([1])] #diagnosis scizophrenia spectrum past
ksads_df= ksads_df[~ksads_df["ksads_4_852_t"].isin([1])] #diagnosis scizophrenia spectrum past

ksads_df.shape, ksads_df.subjectkey.unique().shape

#med history, 0=no, 1=yes
med_df = ksads_df[~ksads_df["medhx_6i"].isin([1])] #head injury
med_df= med_df[~med_df["medhx_6p"].isin([1])] #seizure
med_df= med_df[~med_df["medhx_6j"].isin([1])] #knocked unconscius 
med_df= med_df[~med_df["medhx_2m"].isin([1])] #MS
med_df= med_df[~med_df["medhx_2h"].isin([1])] #epilepsy or seizures
med_df= med_df[~med_df["medhx_2f"].isin([1])] #cerebral palsy
med_df= med_df[~med_df["medhx_2c"].isin([1])] #brain injury

med_df.shape, med_df.subjectkey.unique().shape

#rec for inclusion?, 0 =no, 1 = yes
qc_df = med_df[~med_df["imgincl_rsfmri_include"].isin([0])]
qc_df.shape, qc_df.subjectkey.unique().shape

#drop sibs
subj_df = qc_df.drop_duplicates(subset='rel_family_id', keep='first')

subj_df.shape, subj_df.subjectkey.unique().shape

sublist = subj_df[["subjectkey","interview_date", "interview_age", "sex"]]
sublist.to_csv("sublist.csv")
    
